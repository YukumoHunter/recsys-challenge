{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with the EB-NeRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "from recsys_challenge.utils._constants import (\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")\n",
    "\n",
    "from recsys_challenge.dataset.preprocess.vocab import (setup_word_embedder, build_vocab, build_word_embeddings, build_article_id_to_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"../data/small\")\n",
    "OUTPUT_PATH = Path(\"../data/vocab\")\n",
    "data_split = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_behaviors = pl.scan_parquet(PATH / data_split / \"behaviors.parquet\")\n",
    "df_history = pl.scan_parquet(PATH / data_split / \"history.parquet\")\n",
    "df_articles = pl.scan_parquet(PATH / \"articles.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, word_embedder = setup_word_embedder()\n",
    "\n",
    "df_articles_tok = df_articles.with_columns(\n",
    "    title_tokenized=pl.col(DEFAULT_TITLE_COL).map_elements(lambda x: \" \".join(tokenizer(x)), return_dtype=pl.String)\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be generated, but result isn't used for other stuff\n",
    "_ = build_vocab(\n",
    "    df_behaviors.collect().get_column(DEFAULT_USER_COL), OUTPUT_PATH / \"user_id_vocab.bin\"\n",
    ")\n",
    "\n",
    "articles_vocab = build_vocab(\n",
    "    df_articles_tok.get_column(\"article_id\"), OUTPUT_PATH / \"articles_id_vocab.bin\"\n",
    ")\n",
    "\n",
    "word_vocab = build_vocab(\n",
    "    df_articles_tok.get_column(\"title_tokenized\"), OUTPUT_PATH / \"word_vocab.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_word_embeddings(\n",
    "    word_vocab,\n",
    "    word_embedder,\n",
    "    OUTPUT_PATH / \"word_embeddings.npy\",\n",
    ")\n",
    "\n",
    "build_article_id_to_title(\n",
    "    df_articles_tok,\n",
    "    articles_vocab,\n",
    "    word_vocab,\n",
    "    OUTPUT_PATH / \"article_id_to_title.npy\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
